version: '3.8'

services:
  lecture-qa:
    build: .
    container_name: lecture-qa
    expose:
      - "5000"
    env_file:
      - .env
    environment:
      - FLASK_ENV=production
      - TZ=Asia/Seoul
      # Local Whisper settings (uncomment to enable)
      # - USE_LOCAL_WHISPER=true
      # - WHISPER_MODEL=small  # tiny, base, small, medium, large
    volumes:
      - ./uploads:/app/uploads
      - ./transcripts:/app/transcripts
      - ./data:/app/data
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    # Increase shared memory for PyTorch (needed for Whisper)
    shm_size: '2gb'

  caddy:
    image: caddy:2
    container_name: caddy
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./Caddyfile:/etc/caddy/Caddyfile
      - caddy_data:/data
      - caddy_config:/config
    environment:
      - TZ=Asia/Seoul
    restart: unless-stopped
    depends_on:
      - lecture-qa

volumes:
  caddy_data:
  caddy_config:
